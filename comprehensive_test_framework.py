#!/usr/bin/env python3
"""
Phase 4: ì²´ê³„ì  í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
56ê°œ íŒ¨í„´ ê°„ ì¶©ëŒ ì‹œë‚˜ë¦¬ì˜¤ ì§€ëŠ¥ì  ê²€ì¦ ì‹œìŠ¤í…œ
"""

import sys
import os
import time
import json
from typing import Dict, List, Tuple, Set
from collections import defaultdict
import itertools

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from claude_litellm_proxy.patterns.cloud_patterns import CloudPatterns


class ComprehensiveTestFramework:
    """56ê°œ íŒ¨í„´ì˜ ì²´ê³„ì  ì¶©ëŒ ê²€ì¦ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        self.patterns = CloudPatterns()
        self.all_pattern_names = list(self.patterns._patterns.keys())
        self.conflict_scenarios = []
        self.test_results = {}
        
    def generate_realistic_test_data(self) -> Dict[str, str]:
        """ê° íŒ¨í„´ë³„ ì‹¤ì œ AWS í˜•ì‹ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        return {
            # Priority 50-99: ìƒˆ íŒ¨í„´ë“¤
            'fargate_task': 'arn:aws:ecs:us-east-1:123456789012:task/my-cluster/a1b2c3d4-e5f6-7890-abcd-ef1234567890',
            'ssm_session': 'arn:aws:ssm:us-east-1:123456789012:session/user-12345678901ab',
            'insights_query': 'a1234567-89ab-cdef-0123-456789abcdef',
            'apprunner_service': 'arn:aws:apprunner:us-east-1:123456789012:service/web-app/fedcba0987654321fedcba0987654321',
            'eventbridge_bus': 'arn:aws:events:us-east-1:123456789012:event-bus/custom-business-bus',
            
            # Priority 100-199: ARN íŒ¨í„´ë“¤
            'lambda_arn': 'arn:aws:lambda:us-east-1:123456789012:function:ProcessPayment',
            'ecs_task': 'arn:aws:ecs:us-east-1:123456789012:task/a1b2c3d4-e5f6-7890',
            'elb_arn': 'arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/app/my-lb/1234567890abcdef',
            'iam_role': 'arn:aws:iam::123456789012:role/MyServiceRole',
            'iam_user': 'arn:aws:iam::123456789012:user/alice',
            'kms_key': 'f1234567-89ab-cdef-0123-456789abcdef',
            'cert_arn': 'arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012',
            'secret_arn': 'arn:aws:secretsmanager:us-east-1:123456789012:secret:MySecret-AbCdEf',
            'parameter_arn': 'arn:aws:ssm:us-east-1:123456789012:parameter/myapp/config',
            'codecommit': 'arn:aws:codecommit:us-east-1:123456789012:MyRepository',
            'dynamodb_table': 'arn:aws:dynamodb:us-east-1:123456789012:table/Users',
            'sns_topic': 'arn:aws:sns:us-east-1:123456789012:MyTopic',
            'sqs_queue': 'https://sqs.us-east-1.amazonaws.com/123456789012/MyQueue',
            'stack_id': 'arn:aws:cloudformation:us-east-1:123456789012:stack/MyStack/1a2b3c4d-5e6f-7890-abcd-ef1234567890',
            'kinesis': 'arn:aws:kinesis:us-east-1:123456789012:stream/MyStream',
            'elasticsearch': 'arn:aws:es:us-east-1:123456789012:domain/my-domain',
            'stepfunctions': 'arn:aws:states:us-east-1:123456789012:stateMachine:MyStateMachine',
            'batch_queue': 'arn:aws:batch:us-east-1:123456789012:job-queue/MyJobQueue',
            'athena': 'arn:aws:athena:us-east-1:123456789012:workgroup/primary',
            
            # Priority 200-299: ë¦¬ì†ŒìŠ¤ IDë“¤
            'nat_gateway': 'nat-1234567890abcdef0',
            'ebs_volume': 'vol-1234567890abcdef0',
            'subnet': 'subnet-1234567890abcdef0',
            'vpc': 'vpc-12345678',
            'security_group': 'sg-12345678',
            'ec2_instance': 'i-1234567890abcdef0',
            'ami': 'ami-12345678',
            'efs_id': 'fs-12345678',
            'igw': 'igw-12345678',
            'vpn': 'vpn-12345678',
            'tgw': 'tgw-1234567890abcdef0',
            'snapshot': 'snap-1234567890abcdef0',
            
            # Priority 300-399: ë„¤íŠ¸ì›Œí¬/API
            'api_gateway': 'abcdef1234.execute-api.us-east-1.amazonaws.com',
            'access_key': 'AKIA1234567890ABCDEF',
            'route53_zone': 'Z1234567890ABC',
            'ecr_uri': '123456789012.dkr.ecr.us-east-1.amazonaws.com/my-repo',
            'log_group': '/aws/lambda/my-function',
            
            # Priority 400+: IP ë° ê¸°íƒ€
            'ipv6': '2001:0db8:85a3:0000:0000:8a2e:0370:7334',
            'public_ip': '8.8.8.8',
            'arn': 'arn:aws:s3:::my-bucket',  # Generic ARN
            's3_bucket': 'my-data-bucket',
            's3_logs_bucket': 'my-app-logs',
            'rds_instance': 'my-prod-db',
            'elasticache': 'my-cache-abc12-def',
            'eks_cluster': 'arn:aws:eks:us-east-1:123456789012:cluster/my-cluster',
            'redshift': 'my-redshift-cluster',
            'glue_job': 'glue-job-my-etl',
            'sagemaker': 'arn:aws:sagemaker:us-east-1:123456789012:endpoint/my-endpoint',
            'account_id': '123456789012',
            'session_token': 'FwoGZXIvYXdzEDMaDG9uQ2FsbA1234567890',
            'secret_key': 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
            'cloudfront': 'E1234567890ABC'
        }
    
    def identify_potential_conflicts(self) -> List[Tuple[str, str, str]]:
        """ì¶©ëŒ ê°€ëŠ¥ì„±ì´ ë†’ì€ íŒ¨í„´ ì¡°í•© ì‹ë³„"""
        conflicts = []
        
        # 1. ARN íŒ¨í„´ë“¤ ê°„ì˜ ì¶©ëŒ (Generic ARN vs êµ¬ì²´ì  ARNë“¤)
        arn_patterns = [name for name, pattern in self.patterns._patterns.items() 
                       if 'arn:aws:' in pattern.pattern]
        
        for arn_pattern in arn_patterns:
            if arn_pattern != 'arn':  # Generic ARN ì œì™¸
                conflicts.append((arn_pattern, 'arn', 'arn_vs_specific'))
        
        # 2. UUID í˜•ì‹ íŒ¨í„´ë“¤ ê°„ì˜ ì¶©ëŒ
        uuid_patterns = ['kms_key', 'insights_query']
        for p1, p2 in itertools.combinations(uuid_patterns, 2):
            conflicts.append((p1, p2, 'uuid_format'))
        
        # 3. Account IDì™€ ë‹¤ë¥¸ íŒ¨í„´ë“¤ì˜ í¬í•¨ ê´€ê³„
        patterns_with_account = [name for name, pattern in self.patterns._patterns.items()
                               if r'\d+' in pattern.pattern or '123456789012' in self.generate_realistic_test_data().get(name, '')]
        
        for pattern_name in patterns_with_account[:10]:  # ìƒìœ„ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
            if pattern_name != 'account_id':
                conflicts.append((pattern_name, 'account_id', 'account_inclusion'))
        
        # 4. ê¸¸ì´ê°€ ìœ ì‚¬í•œ ID íŒ¨í„´ë“¤
        similar_length_groups = [
            ['vpc', 'security_group', 'ami', 'efs_id', 'igw', 'vpn'],  # 8ìë¦¬
            ['ec2_instance', 'ebs_volume', 'subnet', 'nat_gateway', 'tgw', 'snapshot'],  # 17ìë¦¬
        ]
        
        for group in similar_length_groups:
            for p1, p2 in itertools.combinations(group, 2):
                conflicts.append((p1, p2, 'similar_length'))
        
        # 5. ë¬¸ìì—´ í¬í•¨ ê´€ê³„
        inclusion_pairs = [
            ('s3_bucket', 's3_logs_bucket', 'string_inclusion'),
            ('rds_instance', 'elb_arn', 'substring_match'),
            ('lambda_arn', 'lambda', 'name_similarity')
        ]
        
        conflicts.extend(inclusion_pairs)
        
        return conflicts
    
    def create_conflict_test_scenarios(self, conflicts: List[Tuple[str, str, str]]) -> List[Dict]:
        """ì¶©ëŒ ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        test_data = self.generate_realistic_test_data()
        scenarios = []
        
        for pattern1, pattern2, conflict_type in conflicts:
            if pattern1 in test_data and pattern2 in test_data:
                # ë‘ íŒ¨í„´ì´ ëª¨ë‘ í¬í•¨ëœ í…ìŠ¤íŠ¸ ìƒì„±
                text = f"Resource A: {test_data[pattern1]} and Resource B: {test_data[pattern2]}"
                
                scenarios.append({
                    'scenario_id': len(scenarios) + 1,
                    'pattern1': pattern1,
                    'pattern2': pattern2,
                    'conflict_type': conflict_type,
                    'test_text': text,
                    'expected_patterns': [pattern1, pattern2]
                })
                
                # Overlap ì‹œë‚˜ë¦¬ì˜¤: ë‘ íŒ¨í„´ì´ ê²¹ì¹˜ëŠ” ê²½ìš°
                if conflict_type in ['arn_vs_specific', 'account_inclusion']:
                    overlap_text = test_data[pattern1]  # í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ì—ì„œ ë‘ íŒ¨í„´ ëª¨ë‘ ë§¤ì¹­ ê°€ëŠ¥
                    scenarios.append({
                        'scenario_id': len(scenarios) + 1,
                        'pattern1': pattern1,
                        'pattern2': pattern2,
                        'conflict_type': f'{conflict_type}_overlap',
                        'test_text': overlap_text,
                        'expected_winner': pattern1 if self.patterns._patterns[pattern1].priority < self.patterns._patterns[pattern2].priority else pattern2
                    })
        
        return scenarios
    
    def run_comprehensive_tests(self) -> Dict:
        """í¬ê´„ì  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ Comprehensive Test Framework - Phase 4")
        print("=" * 60)
        
        # 1. ì ì¬ì  ì¶©ëŒ ì‹ë³„
        print("ğŸ” Step 1: Identifying Potential Conflicts")
        conflicts = self.identify_potential_conflicts()
        print(f"   Found {len(conflicts)} potential conflict pairs")
        
        # 2. í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        print("ğŸ” Step 2: Generating Test Scenarios")
        scenarios = self.create_conflict_test_scenarios(conflicts)
        print(f"   Generated {len(scenarios)} test scenarios")
        
        # 3. ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("ğŸ” Step 3: Running Test Scenarios")
        results = {
            'total_scenarios': len(scenarios),
            'passed': 0,
            'failed': 0,
            'performance_tests': [],
            'conflict_resolutions': [],
            'pattern_statistics': defaultdict(int),
            'detailed_results': []
        }
        
        start_time = time.perf_counter()
        
        for i, scenario in enumerate(scenarios):
            if i % 10 == 0:
                print(f"   Progress: {i}/{len(scenarios)} scenarios")
            
            # ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
            test_result = self._run_single_scenario(scenario)
            results['detailed_results'].append(test_result)
            
            if test_result['passed']:
                results['passed'] += 1
            else:
                results['failed'] += 1
            
            # í†µê³„ ìˆ˜ì§‘
            for pattern in test_result.get('matched_patterns', []):
                results['pattern_statistics'][pattern] += 1
        
        end_time = time.perf_counter()
        results['total_execution_time'] = end_time - start_time
        
        # 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("ğŸ” Step 4: Performance Analysis")
        perf_results = self._run_performance_tests()
        results['performance_tests'] = perf_results
        
        # 5. ê²°ê³¼ ë¶„ì„
        print("ğŸ” Step 5: Results Analysis")
        analysis = self._analyze_results(results)
        results['analysis'] = analysis
        
        return results
    
    def _run_single_scenario(self, scenario: Dict) -> Dict:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
        try:
            matches = self.patterns.find_matches(scenario['test_text'])
            matched_patterns = [m['pattern_name'] for m in matches]
            
            result = {
                'scenario_id': scenario['scenario_id'],
                'conflict_type': scenario['conflict_type'],
                'test_text': scenario['test_text'][:100] + '...' if len(scenario['test_text']) > 100 else scenario['test_text'],
                'matched_patterns': matched_patterns,
                'match_count': len(matches),
                'passed': True,
                'details': {}
            }
            
            # Overlap ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦
            if 'overlap' in scenario['conflict_type']:
                expected_winner = scenario.get('expected_winner')
                if len(matched_patterns) == 1 and matched_patterns[0] == expected_winner:
                    result['details']['overlap_resolution'] = 'correct'
                elif len(matched_patterns) > 1:
                    result['details']['overlap_resolution'] = 'multiple_matches'
                    result['passed'] = False
                else:
                    result['details']['overlap_resolution'] = 'unexpected_winner'
                    result['passed'] = False
            
            return result
            
        except Exception as e:
            return {
                'scenario_id': scenario['scenario_id'],
                'passed': False,
                'error': str(e),
                'test_text': scenario['test_text'][:100] + '...'
            }
    
    def _run_performance_tests(self) -> List[Dict]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        perf_tests = []
        
        # 1. ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        large_text = " ".join([
            self.generate_realistic_test_data()[pattern] 
            for pattern in self.all_pattern_names[:20]
        ] * 10)  # 200ê°œ íŒ¨í„´ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸
        
        start_time = time.perf_counter()
        matches = self.patterns.find_matches(large_text)
        end_time = time.perf_counter()
        
        perf_tests.append({
            'test_name': 'large_text_processing',
            'text_length': len(large_text),
            'match_count': len(matches),
            'processing_time': end_time - start_time,
            'chars_per_second': len(large_text) / (end_time - start_time),
            'passed': (end_time - start_time) < 2.0
        })
        
        # 2. íŒ¨í„´ë³„ ê°œë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_data = self.generate_realistic_test_data()
        pattern_perfs = []
        
        for pattern_name in self.all_pattern_names[:10]:  # ìƒìœ„ 10ê°œ íŒ¨í„´ë§Œ
            if pattern_name in test_data:
                text = test_data[pattern_name] * 100  # 100íšŒ ë°˜ë³µ
                
                start_time = time.perf_counter()
                matches = self.patterns.find_matches(text)
                end_time = time.perf_counter()
                
                pattern_perfs.append({
                    'pattern': pattern_name,
                    'processing_time': end_time - start_time,
                    'matches': len(matches)
                })
        
        perf_tests.append({
            'test_name': 'individual_pattern_performance',
            'results': pattern_perfs
        })
        
        return perf_tests
    
    def _analyze_results(self, results: Dict) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        analysis = {
            'success_rate': results['passed'] / results['total_scenarios'] if results['total_scenarios'] > 0 else 0,
            'most_matched_patterns': dict(sorted(results['pattern_statistics'].items(), key=lambda x: x[1], reverse=True)[:10]),
            'performance_summary': {
                'avg_processing_time': results['total_execution_time'] / results['total_scenarios'],
                'meets_performance_requirements': results['total_execution_time'] < (2.0 * results['total_scenarios'] / 100)
            },
            'recommendations': []
        }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if analysis['success_rate'] < 0.95:
            analysis['recommendations'].append("Some conflict scenarios failed - review overlap detection logic")
        
        if not analysis['performance_summary']['meets_performance_requirements']:
            analysis['recommendations'].append("Performance optimization needed for large-scale scenarios")
        
        # íŒ¨í„´ ì‚¬ìš©ë¥  ë¶„ì„
        unused_patterns = set(self.all_pattern_names) - set(results['pattern_statistics'].keys())
        if unused_patterns:
            analysis['unused_patterns'] = list(unused_patterns)
            analysis['recommendations'].append(f"Consider reviewing {len(unused_patterns)} unused patterns")
        
        return analysis
    
    def print_comprehensive_report(self, results: Dict):
        """ìƒì„¸ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š COMPREHENSIVE TEST RESULTS")
        print("="*60)
        
        # ì „ì²´ ìš”ì•½
        print(f"ğŸ“ˆ Overall Results:")
        print(f"   Total Scenarios: {results['total_scenarios']}")
        print(f"   Passed: {results['passed']} ({results['passed']/results['total_scenarios']*100:.1f}%)")
        print(f"   Failed: {results['failed']} ({results['failed']/results['total_scenarios']*100:.1f}%)")
        print(f"   Execution Time: {results['total_execution_time']:.3f}s")
        
        # ì„±ëŠ¥ ë¶„ì„
        print(f"\nâš¡ Performance Analysis:")
        for perf_test in results['performance_tests']:
            if perf_test['test_name'] == 'large_text_processing':
                print(f"   Large Text Processing:")
                print(f"     - Text Length: {perf_test['text_length']:,} chars")
                print(f"     - Processing Time: {perf_test['processing_time']:.3f}s")
                print(f"     - Performance: {perf_test['chars_per_second']:,.0f} chars/second")
                print(f"     - Status: {'âœ… PASSED' if perf_test['passed'] else 'âŒ FAILED'}")
        
        # íŒ¨í„´ í†µê³„
        print(f"\nğŸ“Š Pattern Usage Statistics (Top 10):")
        for pattern, count in results['analysis']['most_matched_patterns'].items():
            print(f"   {pattern:20s}: {count:3d} matches")
        
        # ê¶Œì¥ì‚¬í•­
        if results['analysis']['recommendations']:
            print(f"\nğŸ’¡ Recommendations:")
            for i, rec in enumerate(results['analysis']['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        # ìµœì¢… í‰ê°€
        success_rate = results['analysis']['success_rate']
        if success_rate >= 0.95:
            print(f"\nğŸ‰ EXCELLENT: {success_rate:.1%} success rate - System is production ready!")
        elif success_rate >= 0.90:
            print(f"\nâœ… GOOD: {success_rate:.1%} success rate - Minor improvements needed")
        else:
            print(f"\nâš ï¸  NEEDS WORK: {success_rate:.1%} success rate - Significant improvements required")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    framework = ComprehensiveTestFramework()
    
    print("Starting comprehensive test framework...")
    print(f"Total patterns to test: {len(framework.all_pattern_names)}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = framework.run_comprehensive_tests()
    
    # ê²°ê³¼ ë³´ê³ ì„œ
    framework.print_comprehensive_report(results)
    
    # JSON íŒŒì¼ë¡œ ìƒì„¸ ê²°ê³¼ ì €ì¥
    with open('comprehensive_test_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Detailed results saved to: comprehensive_test_results.json")
    print("ğŸ‰ Comprehensive testing complete!")


if __name__ == "__main__":
    main()